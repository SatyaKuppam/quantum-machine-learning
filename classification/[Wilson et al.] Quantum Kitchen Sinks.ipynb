{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Kitchen Sinks by Wilson et al.[1]\n",
    "\n",
    "Put simply quantum kitchens sinks(QKS) algorithm allows us to extract high dimensional non-linear transformation of the dataset using a quantum computer. Sometimes it may be the case that, datasets that require non-linear decision boundaries to discriminate, might be solved by less complex methods in higher dimensions$\\dagger$. We can use the high dimensional feature vectors thus created as input to a classical ML algorithm to improve its performance over its purely classical counterpart. Since we are only interested in demonstrating how the feature vectors from QKS give a performance boost, we will use only linear methods in the classical part of the algorithm. This is because we expect the QKS process to be highly non-linear and we run the risk of using a powerful non-linear classical step that will give us good results despite the non-linear quantum transformations. Wilson et al. refer to this as the *Linear Baseline Rule*. Hence we will use *Logistic Regression* as our linear baseline. We will compare the accuracy of classification using logistic regression with and without the embeddings from QKS.\n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "We will be using the (3,5)-MNIST dataset for this demnstration. The dataset consists of a 784-dimensional handwritten examples; with two classes (-1, +1) for 3 and 5 respectively. \n",
    "\n",
    "1. *Standardize the dataset*\n",
    "2.  Use logistic regression to construct a linear baseline.\n",
    "3.  Use the QKS algorithm with the standardized dataset to obtain feature vectors.\n",
    "4.  Use logistic regression with the feature vectors created above to check the performace.\n",
    "\n",
    "### QKS algorithm:\n",
    "Given an example from the dataset we want to encode this in the quantum circuit parameters. We specifically choose the angles of rotation in the quantum circuit of our choice. We then measure the qubits in the Z-basis and the results of the measurements are a part of our feature vector. Since we are interested in high dimensional feature vectors of our dataset, we have two choices either use the quantum circuit with a large number of qubits and the qubits are highly entangled or sample various circuits. For example our MNIST dataset has 784 dimensional vector, a high dimensional feature vectors of the size of 2000-dimensions would need atleast 2000 qubits using the former method. This type of highly entangled quantum circuit with sufficinet depth with reasonable error rates are difficult in the NISQ era, instead we take a small circuit and create multiple parameter sets from the same example, the measurements from each of our quantum circuits forms a part of the feature vector for that example (called episodes). We use a two qubit quantum circuit, with two parameters. Each example is split into two and we derive the parameters from these.\n",
    "\n",
    "Given a dataset $\\{y_i, u_i\\}_{i=1..m}$ of $m$ points where $u_i \\in \\mathbb{R}^p$ and a set of $E$ feature vectors derived from $u$, also called *episodes*[1].\n",
    "$$\\Omega_e \\in \\mathbb{R}^{q \\times p}; \\beta_e \\in \\mathbb{R}^q$$\n",
    "$$\\theta_{i, e} = \\Omega_eu_i + \\beta_e$$. *q* is the number of parameters in our ansatz, which in this case is 2.\n",
    "$\\theta_{i,e}$ are the parameters for the *i-*th example for its *e-*th episode. We simulate the ansatz for each of the $\\theta_{i,e}$ thus forming $q \\times E$ dimensional feature vector for each example. We will use logistic regression to classify these feature vector and compare performance. \n",
    "\n",
    "\n",
    "$\\dagger$: for a much better explanation of this phenomenon please refer to a good ML text like Muphy or Bishop and look up kernel methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirq.contrib.svg import SVGCircuit\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mnist import MNISTData\n",
    "\n",
    "import scipy\n",
    "import cirq\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNISTData(seed)\n",
    "X_train, X_test, y_train, y_test = mnist_data.get_three_five_test_train_split()\n",
    "train_indices = np.random.randint(0, X_train.shape[0], 1000)\n",
    "test_indices = np.random.randint(0, X_test.shape[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784) (100, 784)\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_train[train_indices]\n",
    "y_tr = y_train[train_indices]\n",
    "X_te = X_test[test_indices]\n",
    "y_te = y_test[test_indices]\n",
    "print(X_tr.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(X_tr[0].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Linear Baseline\n",
    "\n",
    "We will use logistic regression to classify 0 and 1 in MNIST dataset to establish the baseline accuracy with a linear model, i.e logistic regression, so that we have a metric to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=seed, max_iter=10000).fit(X_tr, y_tr)\n",
    "# determine accuracy\n",
    "clf.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoQubitMnistQks:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, repetitions=1, episodes=8900, stddev=0.05):\n",
    "        self.qubits = cirq.LineQubit.range(2)\n",
    "        # the number of qubits\n",
    "        self.q = 2\n",
    "        self.p = X_train.shape[1]\n",
    "        self.r = self.p/self.q\n",
    "        self.E = episodes\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.X_test = X_test\n",
    "        self.stddev = stddev\n",
    "        self.repetitions = repetitions\n",
    "        # mask = E x p x q\n",
    "        mask = np.ones((self.E, self.p, self.q))\n",
    "        # this effectively splits the datapoint into two parts\n",
    "        # along a horizontal axis, with the top part for the\n",
    "        # first qubit parameter and the bottom one for the\n",
    "        # second\n",
    "        mask[:, :int(self.r), 0], mask[:, int(self.r):, 1] = 0.0, 0.0\n",
    "        self.mask = mask\n",
    "    \n",
    "    def _get_ansatz(self, theta, draw=False):\n",
    "        circuit = cirq.Circuit()\n",
    "        circuit.append(cirq.rx(theta[0])(self.qubits[0]))\n",
    "        circuit.append(cirq.rx(theta[1])(self.qubits[1]))\n",
    "        circuit.append(cirq.CNOT(self.qubits[0], self.qubits[1]))\n",
    "        circuit.append(cirq.measure(self.qubits[0]))\n",
    "        circuit.append(cirq.measure(self.qubits[1]))\n",
    "        \n",
    "        if draw:\n",
    "            SVGCircuit(circuit)    \n",
    "        return circuit\n",
    "    \n",
    "    def _get_meas(self, theta):\n",
    "        circuit = self._get_ansatz(theta)\n",
    "        result = cirq.Simulator().run(circuit, repetitions=1)\n",
    "        result = np.array([np.sum(result.measurements['0'])/self.repetitions,\n",
    "                           np.sum(result.measurements['1'])/self.repetitions])\n",
    "        return result\n",
    "    \n",
    "    def _get_omega_and_beta(self):\n",
    "        # omega_e = (E x p x q)\n",
    "        omega_e = np.random.normal(0.0, self.stddev, (self.E, self.p,  self.q))\n",
    "        omega = self.mask * omega_e\n",
    "        beta = np.random.uniform(0.0, 2 * np.pi, (self.E, self.q))\n",
    "        return omega, beta\n",
    "    \n",
    "    def _get_params(self, data):\n",
    "        omega, beta = self._get_omega_and_beta()\n",
    "        # params = (n x E x q)\n",
    "        params = data.dot(omega) + beta\n",
    "        return params\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        params_tr = self._get_params(self.X_train)\n",
    "        embeddings_tr = np.zeros((params_tr.shape[0], self.q * self.E))\n",
    "        \n",
    "        params_te = self._get_params(self.X_test)\n",
    "        embeddings_te = np.zeros((params_te.shape[0], self.q * self.E))\n",
    "        \n",
    "        for idx, param in enumerate(params_tr):\n",
    "            if idx % 10 == 0:\n",
    "                print(\"Getting train embeddings for batch: \", idx)\n",
    "            for jdx, theta in enumerate(param):\n",
    "                meas = self._get_meas(theta)\n",
    "                embeddings_tr[idx][jdx] += meas[0]\n",
    "                embeddings_tr[idx][jdx + self.E] += meas[1]\n",
    "        \n",
    "        for idx, param in enumerate(params_te):\n",
    "            if idx % 10 == 0:\n",
    "                print(\"Getting test embeddings for batch: \", idx)\n",
    "            for jdx, theta in enumerate(param):\n",
    "                meas = self._get_meas(theta)\n",
    "                embeddings_te[idx][jdx] += meas[0]\n",
    "                embeddings_te[idx][jdx + self.E] += meas[1]\n",
    "        return embeddings_tr, embeddings_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoQubitMnistQks = TwoQubitMnistQks(X_tr, y_tr, X_te, y_te, episodes=1000, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow: auto; white-space: pre;\">0: ───Rx(π)───@───M───\n",
       "              │\n",
       "1: ───Rx(π)───X───M───</pre>"
      ],
      "text/plain": [
       "0: ───Rx(π)───@───M───\n",
       "              │\n",
       "1: ───Rx(π)───X───M───"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoQubitMnistQks._get_ansatz([np.pi, np.pi], draw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train embeddings for batch:  0\n",
      "Getting train embeddings for batch:  10\n",
      "Getting train embeddings for batch:  20\n",
      "Getting train embeddings for batch:  30\n",
      "Getting train embeddings for batch:  40\n",
      "Getting train embeddings for batch:  50\n",
      "Getting train embeddings for batch:  60\n",
      "Getting train embeddings for batch:  70\n",
      "Getting train embeddings for batch:  80\n",
      "Getting train embeddings for batch:  90\n",
      "Getting train embeddings for batch:  100\n",
      "Getting train embeddings for batch:  110\n",
      "Getting train embeddings for batch:  120\n",
      "Getting train embeddings for batch:  130\n",
      "Getting train embeddings for batch:  140\n",
      "Getting train embeddings for batch:  150\n",
      "Getting train embeddings for batch:  160\n",
      "Getting train embeddings for batch:  170\n",
      "Getting train embeddings for batch:  180\n",
      "Getting train embeddings for batch:  190\n",
      "Getting train embeddings for batch:  200\n",
      "Getting train embeddings for batch:  210\n",
      "Getting train embeddings for batch:  220\n",
      "Getting train embeddings for batch:  230\n",
      "Getting train embeddings for batch:  240\n",
      "Getting train embeddings for batch:  250\n",
      "Getting train embeddings for batch:  260\n",
      "Getting train embeddings for batch:  270\n",
      "Getting train embeddings for batch:  280\n",
      "Getting train embeddings for batch:  290\n",
      "Getting train embeddings for batch:  300\n",
      "Getting train embeddings for batch:  310\n",
      "Getting train embeddings for batch:  320\n",
      "Getting train embeddings for batch:  330\n",
      "Getting train embeddings for batch:  340\n",
      "Getting train embeddings for batch:  350\n",
      "Getting train embeddings for batch:  360\n",
      "Getting train embeddings for batch:  370\n",
      "Getting train embeddings for batch:  380\n",
      "Getting train embeddings for batch:  390\n",
      "Getting train embeddings for batch:  400\n",
      "Getting train embeddings for batch:  410\n",
      "Getting train embeddings for batch:  420\n",
      "Getting train embeddings for batch:  430\n",
      "Getting train embeddings for batch:  440\n",
      "Getting train embeddings for batch:  450\n",
      "Getting train embeddings for batch:  460\n",
      "Getting train embeddings for batch:  470\n",
      "Getting train embeddings for batch:  480\n",
      "Getting train embeddings for batch:  490\n",
      "Getting train embeddings for batch:  500\n",
      "Getting train embeddings for batch:  510\n",
      "Getting train embeddings for batch:  520\n",
      "Getting train embeddings for batch:  530\n",
      "Getting train embeddings for batch:  540\n",
      "Getting train embeddings for batch:  550\n",
      "Getting train embeddings for batch:  560\n",
      "Getting train embeddings for batch:  570\n",
      "Getting train embeddings for batch:  580\n",
      "Getting train embeddings for batch:  590\n",
      "Getting train embeddings for batch:  600\n",
      "Getting train embeddings for batch:  610\n",
      "Getting train embeddings for batch:  620\n",
      "Getting train embeddings for batch:  630\n",
      "Getting train embeddings for batch:  640\n",
      "Getting train embeddings for batch:  650\n",
      "Getting train embeddings for batch:  660\n",
      "Getting train embeddings for batch:  670\n",
      "Getting train embeddings for batch:  680\n",
      "Getting train embeddings for batch:  690\n",
      "Getting train embeddings for batch:  700\n",
      "Getting train embeddings for batch:  710\n",
      "Getting train embeddings for batch:  720\n",
      "Getting train embeddings for batch:  730\n",
      "Getting train embeddings for batch:  740\n",
      "Getting train embeddings for batch:  750\n",
      "Getting train embeddings for batch:  760\n",
      "Getting train embeddings for batch:  770\n",
      "Getting train embeddings for batch:  780\n",
      "Getting train embeddings for batch:  790\n",
      "Getting train embeddings for batch:  800\n",
      "Getting train embeddings for batch:  810\n",
      "Getting train embeddings for batch:  820\n",
      "Getting train embeddings for batch:  830\n",
      "Getting train embeddings for batch:  840\n",
      "Getting train embeddings for batch:  850\n",
      "Getting train embeddings for batch:  860\n",
      "Getting train embeddings for batch:  870\n",
      "Getting train embeddings for batch:  880\n",
      "Getting train embeddings for batch:  890\n",
      "Getting train embeddings for batch:  900\n",
      "Getting train embeddings for batch:  910\n",
      "Getting train embeddings for batch:  920\n",
      "Getting train embeddings for batch:  930\n",
      "Getting train embeddings for batch:  940\n",
      "Getting train embeddings for batch:  950\n",
      "Getting train embeddings for batch:  960\n",
      "Getting train embeddings for batch:  970\n",
      "Getting train embeddings for batch:  980\n",
      "Getting train embeddings for batch:  990\n",
      "Getting test embeddings for batch:  0\n",
      "Getting test embeddings for batch:  10\n",
      "Getting test embeddings for batch:  20\n",
      "Getting test embeddings for batch:  30\n",
      "Getting test embeddings for batch:  40\n",
      "Getting test embeddings for batch:  50\n",
      "Getting test embeddings for batch:  60\n",
      "Getting test embeddings for batch:  70\n",
      "Getting test embeddings for batch:  80\n",
      "Getting test embeddings for batch:  90\n"
     ]
    }
   ],
   "source": [
    "embeddings_tr, embeddings_te = twoQubitMnistQks.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=seed, max_iter=1000).fit(embeddings_tr, y_tr)\n",
    "clf.score(embeddings_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1] Wilson, C.M., Otterbach, J.S., Tezak, N., Smith, R.S., Polloreno, A.M., Karalekas, P.J., Heidel, S., Alam, M.S., Crooks, G.E. and da Silva, M.P., 2018. Quantum kitchen sinks: An algorithm for machine learning on near-term quantum computers. arXiv preprint arXiv:1806.08321."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
